import os
from typing import Optional
from google import genai
from llm_engine.base import BaseLLMProvider


class GeminiClient(BaseLLMProvider):
    '''
    LLM provider using Google Gemini API.

    This provider integrates with Google's Gemini models via
    the official GenAI SDK and supports system and user prompts
    in a provider-agnostic manner.
    '''

    def __init__(
        self,
        user_prompt_template: str,
        system_prompt: Optional[str] = None,
        model: str = 'gemini-1.5-flash',
    ):
        '''
        Initialize the Gemini provider.

        Parameters
        ----------
        user_prompt_template : str
            Template string used to generate user prompts.
        system_prompt : str, optional
            System-level instruction defining model behavior.
        model : str, default='gemini-1.5-flash'
            Name of the Gemini model to use.

        Raises
        ------
        ValueError
            If the GEMINI_API_KEY environment variable is not set.
        '''
        super().__init__(
            user_prompt_template=user_prompt_template,
            system_prompt=system_prompt,
        )

        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError(
                'GEMINI_API_KEY not set in environment variables!'
            )

        self.client = genai.Client(api_key=api_key)
        self.model = model

    def generate(
        self,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0
    ) -> str:
        '''
        Generate a response from the Gemini model.

        Gemini does not natively support role-based messages
        in the same way as OpenAI or Ollama. Therefore, the
        system prompt is prepended to the user prompt in a
        controlled and explicit format.

        Parameters
        ----------
        user_prompt : str
            Rendered user prompt.
        system_prompt : str, optional
            System-level instruction.

        Returns
        -------
        str
            Raw text output generated by the Gemini model.
        '''
        response = self.client.models.generate_content(
            model=self.model,
            contents=user_prompt,
            config=genai.types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=temperature
            ),
        )

        if response.text is None:
            raise ValueError("Gemini returned empty response")

        return response.text
