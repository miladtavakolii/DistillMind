from abc import ABC, abstractmethod
from typing import Any, Optional
import json
import re
import json


class BaseLLMProvider(ABC):
    '''
    Abstract base class for LLM providers.

    This class defines the shared logic for:
        - Loading and storing prompt templates
        - Building formatted prompts for a given article
        - Extracting and validating JSON output from LLM responses

    Subclasses (e.g., Ollama, Gemini, OpenAI) must implement the `generate` method
    responsible for sending prompts to their specific model backends.

    Attributes
    ----------
    prompt_template:
        The raw prompt template loaded from configuration or file.
        It may contain placeholders to be filled using `build_prompt`.
    system_prompt:
        System-level instruction defining the model's role,
        behavior, and output constraints.
    '''

    def __init__(self,
                 user_prompt_template: str,
                 system_prompt: Optional[str] = None,
                 ):
        '''
        Initialize the provider with a prompt template.

        Parameters
        ----------
        user_prompt_template : str
            Template string used to generate the user prompt.
            Can contain placeholders such as `{text}`.
        system_prompt : str, optional
            System-level instruction defining the model's role,
            behavior, and output constraints.
        '''
        self.user_prompt_template = user_prompt_template
        self.system_prompt = system_prompt

    @abstractmethod
    def generate(self,
                 user_prompt: str,
                 system_prompt: Optional[str] = None,
                 temperature: float = 0
                 ) -> str:
        '''
        Send the input prompt to the model and return the raw (unprocessed)
        string output from the LLM.

        Parameters
        ----------
        user_prompt : str
            Fully rendered user prompt.
        system_prompt : str, optional
            System instruction passed to the model.

        Returns
        -------
        str
            Raw text output generated by the model.
        '''
        pass

    def build_prompt(self, **kwargs: Any) -> str:
        '''
        Construct the final prompt by filling template placeholders.

        Parameters
        ----------
        **kwargs : Any
            Values used to fill the prompt template.

        Returns
        -------
        str
            Rendered user prompt string.
        '''
        return self.user_prompt_template.format(**kwargs)

    def execute(self, temperature: float, **fields: Any) -> dict:
        '''
        High-level workflow for get responce from LLM provider.

        Steps:
            1. Construct prompt using fields
            2. Generate raw model output via `generate`
            3. Extract and validate JSON sentiment result

        Parameters
        ----------
        **fields:
            text content required to fill the prompt.

        Returns
        -------
        Structured sentiment analysis result.
        '''
        user_prompt = self.build_prompt(**fields)
        raw_output = self.generate(
            user_prompt=user_prompt,
            system_prompt=self.system_prompt,
            temperature=temperature
        )

        return raw_output
