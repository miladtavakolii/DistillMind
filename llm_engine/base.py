from abc import ABC, abstractmethod
from typing import Any, Optional
import json
import re
import json


class BaseLLMProvider(ABC):
    '''
    Abstract base class for sentiment-analysis LLM providers.

    This class defines the shared logic for:
        - Loading and storing prompt templates
        - Building formatted prompts for a given article
        - Extracting and validating JSON output from LLM responses

    Subclasses (e.g., Ollama, Gemini, OpenAI) must implement the `generate` method
    responsible for sending prompts to their specific model backends.

    Attributes
    ----------
    prompt_template:
        The raw prompt template loaded from configuration or file.
        It may contain placeholders to be filled using `build_prompt`.
    '''

    def __init__(self,
                 user_prompt_template: str,
                 system_prompt: Optional[str] = None,
                 ):
        '''
        Initialize the provider with a prompt template.

        Parameters
        ----------
        user_prompt_template : str
            Template string used to generate the user prompt.
            Can contain placeholders such as `{text}`.
        system_prompt : str, optional
            System-level instruction defining the model's role,
            behavior, and output constraints.
        '''
        self.user_prompt_template = user_prompt_template
        self.system_prompt = system_prompt

    @abstractmethod
    def generate(self,
                 user_prompt: str,
                 system_prompt: Optional[str] = None,
                 temperature: float = 0
                 ) -> str:
        '''
        Send the input prompt to the model and return the raw (unprocessed)
        string output from the LLM.

        Parameters
        ----------
        user_prompt : str
            Fully rendered user prompt.
        system_prompt : str, optional
            System instruction passed to the model.

        Returns
        -------
        str
            Raw text output generated by the model.
        '''
        pass

    def build_prompt(self, **kwargs: Any) -> str:
        '''
        Construct the final prompt by filling template placeholders.

        Parameters
        ----------
        **kwargs : Any
            Values used to fill the prompt template.

        Returns
        -------
        str
            Rendered user prompt string.
        '''
        return self.user_prompt_template.format(**kwargs)

    def extract_json(self, text: str) -> dict:
        '''
        Extract and parse a valid JSON object from arbitrary LLM output.

        This method:
            - Removes markdown code fences (```json ... ```)
            - Normalizes non-standard quotes
            - Attempts full-text JSON parsing
            - Falls back to regex-based JSON extraction when needed

        Parameters
        ----------
        text: Raw output returned by the LLM.

        Returns
        -------
        Parsed JSON object extracted from the model output.

        Raises
        ------
        ValueError:
            If no valid JSON structure can be recovered.
        '''
        text = text.replace('â€', '\'')
        # Remove markdown code fences
        text = text.strip()
        text = re.sub(r"^```[a-zA-Z]*", "", text)
        text = re.sub(r"```$", "", text)
        text = text.strip()

        # Try parse as JSON directly
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # If JSON is inside text somewhere:
        match = re.search(r"\{[\s\S]*\}", text)
        if match:
            return json.loads(match.group(0))

        raise ValueError(f'[ERROR] No valid JSON found in model output:\n {text}')

    def analyze(self, temperature: float, **fields: Any) -> dict:
        '''
        High-level workflow for performing sentiment analysis with an LLM provider.

        Steps:
            1. Construct prompt using fields
            2. Generate raw model output via `generate`
            3. Extract and validate JSON sentiment result

        Parameters
        ----------
        **fields:
            text content required to fill the prompt.

        Returns
        -------
        Structured sentiment analysis result.

        Raises
        ------
        ValueError:
            If the model returns invalid or unparsable JSON.
        '''
        user_prompt = self.build_prompt(**fields)
        raw_output = self.generate(
            user_prompt=user_prompt,
            system_prompt=self.system_prompt,
            temperature=temperature
        )

        try:
            return self.extract_json(raw_output)
        except json.JSONDecodeError:
            raise ValueError(
                f'[ERROR] Provider returned invalid JSON:\n{raw_output}')
